{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "871370e9",
   "metadata": {},
   "source": [
    "This jupyter notebook is an example of processing of the yellowfin Autonomous surface vehicle. \n",
    "\n",
    "Its written by Spicer Bak, July 2023\n",
    "\n",
    "\n",
    "Outline is as follows: \n",
    "`- first define a library of functions from which to call\n",
    " - then load \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfff36ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# called yellowfinlib \n",
    "import os\n",
    "import struct\n",
    "import datetime as DT\n",
    "import h5py\n",
    "import netCDF4 as nc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "def read_emlid_pos(fldrlistPPK, plot=False, saveFname=None):\n",
    "    \"\"\"read and parse multiple pos files in multiple folders provided\n",
    "    \n",
    "    :param fldrlistPPK: list of folders to provide\n",
    "    :param plot: if a path name will save a QA/QC plots (default=False)\n",
    "    :param saveFname: will save file as h5\n",
    "    :return: dataframe with loaded data from pos file\n",
    "    \"\"\"\n",
    "    T_ppk = pd.DataFrame()\n",
    "    for fldr in sorted(fldrlistPPK):\n",
    "        # this is before ppk processing so should agree with nmea strings\n",
    "        fn = glob.glob(os.path.join(fldr, \"*.pos\"))[0]\n",
    "        try:\n",
    "            colNames = ['datetime', 'lat', 'lon', 'height', 'Q', 'ns', 'sdn(m)',  'sde(m)', 'sdu(m)',\n",
    "                        'sdne(m)', 'sdeu(m)',  'sdun(m)', 'age(s)',  'ratio']\n",
    "            Tpos = pd.read_csv(fn, delimiter=r'\\s+ ', header=10, names=colNames, engine='python')\n",
    "            print(f'loaded {fn}')\n",
    "            if all(Tpos.iloc[-1]):  #if theres nan's in the last row\n",
    "                Tpos = Tpos.iloc[:-1] # remove last row\n",
    "            T_ppk = pd.concat([T_ppk, Tpos]) # merge multiple files to single dataframe\n",
    "        \n",
    "        except:\n",
    "            continue\n",
    "    T_ppk['datetime'] = pd.to_datetime(T_ppk['datetime'], format='%Y/%m/%d %H:%M:%S.%f')\n",
    "    \n",
    "    # now make plot of both files\n",
    "    # first llh file\n",
    "    # plt.plot(T_LLH['lon'], T_LLH['lat'], '.-m', label = 'LLH file')\n",
    "    # plt.xlabel('longitude')\n",
    "    # plt.ylabel('latitude')\n",
    "    if plot is not False:\n",
    "        plt.plot(T_ppk['lon'], T_ppk['lat'], '.-g', label = 'PPK file')\n",
    "        plt.xlabel('longitude')\n",
    "        plt.ylabel('latitude')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot+'Lat_Lon')\n",
    "        plt.close()\n",
    "        \n",
    "        fig = plt.figure()\n",
    "        plt.plot(T_ppk['datetime'], T_ppk['height'], label='elevation')\n",
    "        plt.plot(T_ppk['datetime'], 10 * T_ppk['Q'], '.', label='quality factor')\n",
    "        plt.plot(T_ppk['datetime'], 10000 * (T_ppk['lat'] - T_ppk['lat'].iloc[0]), label='lat from original lat')\n",
    "        plt.xlabel('time')\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(plot+'elev_Q')\n",
    "        plt.close()\n",
    "    \n",
    "    return T_ppk\n",
    "\n",
    "\n",
    "def loadSonar_s500_binary(dataPath, outfname=None):\n",
    "    \"\"\"Loads and concatenates all of the binary files (*.dat) located in the dataPath location\n",
    "    \n",
    "    :param dataPath: search path for sonar data files\n",
    "    :param outfname: string to save h5 file. If None it will skip this process. (Default =None)\n",
    "    :return: pandas data frame of sonar data\n",
    "    \"\"\"\n",
    "    dd = glob.glob(os.path.join(dataPath, \"*.dat\"))  # find dat files for sonar\n",
    "    print(f'found {len(dd)} sonar files for procesing')  # loop through files\n",
    "    # https://docs.ceruleansonar.com/c/v/s-500-sounder/appendix-f-programming-api\n",
    "    ij, i3 = 0, 0\n",
    "    allocateSize = 50000  # some rediculously large number that memory can still hold.\n",
    "    # initialize variables for loop\n",
    "    distance, confidence, transmit_duration = np.zeros(allocateSize), np.zeros(allocateSize), np.zeros(\n",
    "        allocateSize)  # [],\n",
    "    ping_number, scan_start, scan_length = np.zeros(allocateSize), np.zeros(allocateSize), np.zeros(allocateSize)\n",
    "    end_ping_hz, adc_sample_hz, timestamp_msec, spare2 = np.zeros(allocateSize), np.zeros(allocateSize), \\\n",
    "                                                         np.zeros(allocateSize), np.zeros(allocateSize)\n",
    "    start_mm, length_mm, start_ping_hz = np.zeros(allocateSize), np.zeros(allocateSize), np.zeros(allocateSize),\n",
    "    ping_duration_sec, analog_gain, profile_data_length, = np.zeros(allocateSize), np.zeros(allocateSize), np.zeros(\n",
    "        allocateSize)\n",
    "    \n",
    "    min_pwr, step_db, smooth_depth_m, fspare2 = np.zeros(allocateSize), np.zeros(allocateSize), \\\n",
    "                                                np.zeros(allocateSize), np.zeros(allocateSize)\n",
    "    is_db, gain_index, power_results = np.zeros(allocateSize), np.zeros(allocateSize), np.zeros(allocateSize)\n",
    "    max_pwr, num_results = np.zeros(allocateSize), np.zeros(allocateSize, dtype=int)\n",
    "    gain_setting, decimation, reserved = np.zeros(allocateSize), np.zeros(allocateSize), np.zeros(allocateSize),\n",
    "    # these are complicated preallocations\n",
    "    txt, dt_profile, dt_txt, dt = np.zeros(allocateSize, dtype=object), np.zeros(allocateSize, dtype=object), \\\n",
    "                                  np.zeros(allocateSize, dtype=object), np.zeros(allocateSize, dtype=object)\n",
    "    rangev = np.zeros((allocateSize*2, allocateSize)) #arbitrary large value for time\n",
    "    profile_data = np.zeros((allocateSize*2, allocateSize))\n",
    "    \n",
    "    for fi in tqdm.tqdm(range(len(dd))):\n",
    "        with open(dd[fi], 'rb') as fid:\n",
    "            fname = dd[fi]\n",
    "            print(f'processing {fname}')\n",
    "            xx = fid.read()\n",
    "            st = [i + 1 for i in range(len(xx)) if xx[i:i + 2] == b'BR']\n",
    "            # initalize variables for loop\n",
    "            packet_len, packet_id = np.zeros(len(st)), np.zeros(len(st))\n",
    "            for ii in range(len(st) - 1):\n",
    "                fid.seek(st[ii] + 1, os.SEEK_SET)\n",
    "                datestring = fid.read(26).decode('utf-8',\n",
    "                                                 'replace')  # 'replace' causes a replacement marker (such as '?')\n",
    "                # to be inserted where there is malformed data.\n",
    "                try:\n",
    "                    dt[ii] = DT.datetime.strptime(datestring, '%Y-%m-%d %H:%M:%S.%f')\n",
    "                except:\n",
    "                    continue\n",
    "                packet_len[ii] = struct.unpack('<H', fid.read(2))[0]\n",
    "                packet_id[ii] = struct.unpack('<H', fid.read(2))[0]\n",
    "                r1 = struct.unpack('<B', fid.read(1))[0]\n",
    "                r2 = struct.unpack('<B', fid.read(1))[0]\n",
    "                if packet_id[ii] == 1300: # these are i believe ping sonar values\n",
    "                    distance[ij] = struct.unpack('<I', fid.read(4))[0]  # mm\n",
    "                    confidence[ij] = struct.unpack('<H', fid.read(2))[0]  # mm\n",
    "                    transmit_duration[ij] = struct.unpack('<H', fid.read(2))[0]  # us\n",
    "                    ping_number[ij] = struct.unpack('<I', fid.read(4))[0]  # #\n",
    "                    scan_start[ij] = struct.unpack('<I', fid.read(4))[0]  # mm\n",
    "                    scan_length[ij] = struct.unpack('<I', fid.read(4))[0]  # mm\n",
    "                    gain_setting[ij] = struct.unpack('<I', fid.read(4))[0]\n",
    "                    profile_data_length[ij] = struct.unpack('<I', fid.read(4))[0]\n",
    "                    for jj in range(200):\n",
    "                        tmp = struct.unpack('<B', fid.read(1))[0]\n",
    "                        if tmp:\n",
    "                            profile_data[ij, jj] = tmp\n",
    "                    ij += 1\n",
    "                \n",
    "                if packet_id[ii] == 3:\n",
    "                    txt[ij] = fid.read(int(packet_len[ii])).decode('utf-8')\n",
    "                    dt_txt[ij] = dt\n",
    "                if packet_id[ii] == 1308: # these are s500 protocols\n",
    "                    dtp = dt\n",
    "                    # https://docs.ceruleansonar.com/c/v/s-500-sounder/appendix-f-programming-api#ping-response-packets\n",
    "                    ping_number[ij] = struct.unpack('<I', fid.read(4))[0]  # mm\n",
    "                    start_mm[ij] = struct.unpack('<I', fid.read(4))[0]  # mm\n",
    "                    length_mm[ij] = struct.unpack('<I', fid.read(4))[0]  # mm\n",
    "                    start_ping_hz[ij] = struct.unpack('<I', fid.read(4))[0]  # us\n",
    "                    end_ping_hz[ij] = struct.unpack('<I', fid.read(4))[0]  # #\n",
    "                    adc_sample_hz[ij] = struct.unpack('<I', fid.read(4))[0]  # mm\n",
    "                    timestamp_msec[ij] = struct.unpack('I', fid.read(4))[0]\n",
    "                    spare2[ij] = struct.unpack('I', fid.read(4))[0]\n",
    "                    \n",
    "                    ping_duration_sec[ij] = struct.unpack('f', fid.read(4))[0]\n",
    "                    analog_gain[ij] = struct.unpack('f', fid.read(4))[0]\n",
    "                    max_pwr[ij] = struct.unpack('f', fid.read(4))[0]\n",
    "                    min_pwr[ij] = struct.unpack('f', fid.read(4))[0]\n",
    "                    step_db[ij] = struct.unpack('f', fid.read(4))[0]\n",
    "                    smooth_depth_m[ij] = struct.unpack('f', fid.read(4))[0]\n",
    "                    fspare2[ij] = struct.unpack('f', fid.read(4))[0]\n",
    "                    \n",
    "                    is_db[ij] = struct.unpack('B', fid.read(1))[0]\n",
    "                    gain_index[ij] = struct.unpack('B', fid.read(1))[0]\n",
    "                    decimation[ij] = struct.unpack('B', fid.read(1))[0]\n",
    "                    reserved[ij] = struct.unpack('B', fid.read(1))[0]\n",
    "                    num_results[ij] = struct.unpack('H', fid.read(2))[0]\n",
    "                    power_results[ij] = struct.unpack('H', fid.read(2))[0]\n",
    "                    rangev[ij, 0:num_results[ij]] = np.linspace(start_mm[ij], start_mm[ij] + length_mm[ij],\n",
    "                                                                num_results[ij])\n",
    "                    dt_profile[ij] = dt[ii]  # assign datetime from data written\n",
    "                    # profile_data_single = [] #= np.empty((num_results[-1], ), dtype=np.uint16)\n",
    "                    for jj in range(num_results[ij]):\n",
    "                        # print(jj)\n",
    "                        read = fid.read(2)\n",
    "                        if read:\n",
    "                            try:  # data should be unsigned short\n",
    "                                tmp = struct.unpack('<H', read)[0]\n",
    "                            except:  # when it's unsigned character\n",
    "                                tmp = struct.unpack('B', read)[0]\n",
    "                            if tmp:\n",
    "                                profile_data[ij, jj] = tmp\n",
    "                    ij += 1\n",
    "    \n",
    "    # clean up array's from over allocation to free up memory and data\n",
    "    idxShort = (num_results != 0 ).sum() #np.argwhere(num_results != 0).max()  # identify index for end of data to keep\n",
    "    num_results = np.median(num_results[:idxShort]).astype(int) #num_results[:idxShort][0]\n",
    "\n",
    "    # make data frame for output\n",
    "    \n",
    "    smooth_depth_m = smooth_depth_m[:idxShort]\n",
    "    reserved = reserved[:idxShort]\n",
    "    start_mm = start_mm[:idxShort]\n",
    "    length_mm = length_mm[:idxShort]\n",
    "    start_ping_hz = start_ping_hz[:idxShort]\n",
    "    end_ping_hz = end_ping_hz[:idxShort]\n",
    "    adc_sample_hz = adc_sample_hz[:idxShort]\n",
    "    timestamp_msec = timestamp_msec[:idxShort]\n",
    "    spare2 = spare2[:idxShort]\n",
    "    ping_duration_sec = ping_duration_sec[:idxShort]\n",
    "    analog_gain = analog_gain[:idxShort]\n",
    "    max_pwr = max_pwr[:idxShort]\n",
    "    min_pwr = min_pwr[:idxShort]\n",
    "    step_db = step_db[:idxShort]\n",
    "    fspare2 = fspare2[:idxShort]\n",
    "    is_db = is_db[:idxShort]\n",
    "    gain_index = gain_index[:idxShort]\n",
    "    decimation = decimation[:idxShort]\n",
    "    dt_profile = dt_profile[:idxShort]\n",
    "    \n",
    "    # rangev,  profile_data need to be handled separately\n",
    "    rangev = rangev[0, :num_results]\n",
    "    profile_data = profile_data[:idxShort, :num_results].T\n",
    "    \n",
    "    # now save output file (can't save as pandas because of multi-dimensional sonar data)\n",
    "    if outfname is not None:\n",
    "        with h5py.File(outfname, 'w') as hf:\n",
    "            hf.create_dataset('min_pwr', data=min_pwr)\n",
    "            hf.create_dataset('ping_duration', data=ping_duration_sec)\n",
    "            hf.create_dataset('time', data=nc.date2num(dt_profile, 'seconds since 1970-01-01'))\n",
    "            hf.create_dataset('smooth_depth_m', data=smooth_depth_m)\n",
    "            hf.create_dataset('profile_data', data=profile_data)  # putting time as first axis\n",
    "            hf.create_dataset('num_results', data=num_results)\n",
    "            hf.create_dataset('start_mm', data=start_mm)\n",
    "            hf.create_dataset('length_mm', data=length_mm)\n",
    "            hf.create_dataset('start_ping_hz', data=start_ping_hz)\n",
    "            hf.create_dataset('end_ping_hz', data=end_ping_hz)\n",
    "            hf.create_dataset('adc_sample_hz', data=adc_sample_hz)\n",
    "            hf.create_dataset('timestamp_msec', data=timestamp_msec)\n",
    "            hf.create_dataset('analog_gain', data=analog_gain)\n",
    "            hf.create_dataset('max_pwr', data=max_pwr)\n",
    "            hf.create_dataset('this_ping_depth_m', data=step_db)\n",
    "            hf.create_dataset('this_ping_depth_measurement_confidence', data=is_db)\n",
    "            hf.create_dataset('smoothed_depth_measurement_confidence', data=reserved)\n",
    "            hf.create_dataset('gain_index', data=gain_index)\n",
    "            hf.create_dataset('decimation', data=decimation)\n",
    "            hf.create_dataset('range_m', data=rangev / 1000)\n",
    "    \n",
    "\n",
    "def load_h5_to_dictionary(fname):\n",
    "    \"\"\"Loads already created H5 file from the sonar data. \"\"\"\n",
    "    hf = h5py.File(fname, 'r')\n",
    "    dataOut = {}\n",
    "    for key in hf.keys():\n",
    "        dataOut[key] = np.array(hf.get(key))\n",
    "    return dataOut\n",
    "\n",
    "\n",
    "def plot_single_backscatterProfile(fname, time, sonarRange, profile_data, this_ping_depth_m, smooth_depth_m, index):\n",
    "    \"\"\"Create's a plot that shows full backscatter and individual profile  with identified depths\n",
    "    \n",
    "    :param fname:\n",
    "    :param time:\n",
    "    :param sonarRange:\n",
    "    :param profile_data:\n",
    "    :param this_ping_depth_m:\n",
    "    :param smooth_depth_m:\n",
    "    :param index:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    ax1 = plt.subplot2grid((4, 4), (0, 1), colspan=3, rowspan=4)\n",
    "    backscatter = ax1.pcolormesh(time, sonarRange[0], profile_data.T, shading='auto')\n",
    "    ax1.plot(time, this_ping_depth_m, color='black', ms=0.1, label='instant depth', alpha=0.5)\n",
    "    # ax1.plot(time, smooth_depth_m, 'black', ms=1, label='Smooth Depth')\n",
    "    ax1.plot(time[index], smooth_depth_m[index], ms=15, marker='X', color='red')\n",
    "    cbar = plt.colorbar(mappable=backscatter, ax=ax1)\n",
    "    cbar.set_label('backscatter value')\n",
    "    ax1.set_ylim([0, 5])\n",
    "    \n",
    "    ax2 = plt.subplot2grid((4, 4), (0, 0), rowspan=4, sharey=ax1)\n",
    "    ax2.plot(profile_data[index], sonarRange[0], alpha=1)\n",
    "    ax2.plot(profile_data[index, np.argmin(np.abs(sonarRange[index] - this_ping_depth_m[index]))], this_ping_depth_m[\n",
    "        index], 'grey', marker='X', ms=10, label='this ping')\n",
    "    ax2.plot(profile_data[index, np.argmin(np.abs(sonarRange[index] - smooth_depth_m[index]))], smooth_depth_m[index],\n",
    "             'black', marker='X', ms=10, label='smoothed bottom')\n",
    "    ax2.legend()\n",
    "    \n",
    "    for ii in range(5):\n",
    "        ax2.plot(profile_data[index - ii], sonarRange[0], alpha=.4 - ii * .07, color='k')\n",
    "    ax2.set_ylabel('depth [m]')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(fname)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def mLabDatetime_to_epoch(dt):\n",
    "    \"\"\"Convert matlab datetime to unix Epoch time\"\"\"\n",
    "    epoch = DT.datetime(1970, 1, 1)\n",
    "    delta = dt - epoch\n",
    "    return delta.total_seconds()\n",
    "\n",
    "def convertEllipsoid2NAVD88(lats, lons, ellipsoids, geoidFile='g2012bu8.bin'):\n",
    "    \"\"\"converts elipsoid values to NAVD88's\n",
    "   \n",
    "   NOTE: if this is the first time you're using this, you'll likely have to go get the geoid bin file.  Code was\n",
    "        developed using the uncompressed bin file.  It is unclear if the pygeodesy library requires the bin file to\n",
    "        be uncompressed.  https://geodesy.noaa.gov/GEOID/GEOID12B/GEOID12B_CONUS.shtml\n",
    "  \n",
    "  :param lats:\n",
    "  :param lons:\n",
    "  :param ellipsoids: raw sattelite elipsoid values.\n",
    "  :param geoidFile: pull from https://geodesy.noaa.gov/GEOID/GEOID12B/GEOID12B_CONUS.shtml\n",
    "  :return: NAVD88 values\n",
    "  \"\"\"\n",
    "    from pygeodesy import geoids\n",
    "    assert len(lons) == len(lats) == len(ellipsoids), 'lons/lats/elipsoids need to be of same length'\n",
    "    try:\n",
    "        instance = geoids.GeoidG2012B(geoidFile)\n",
    "    except ImportError:\n",
    "        print(\"if this is the first time you're using this, you'll likely have to go get the geoid bin file.  Code was \"\n",
    "              \"developed using the uncompressed bin file.  It is unclear if the pygeodesy library requires the bin file to\"\n",
    "              \" be uncompressed.  https://geodesy.noaa.gov/GEOID/GEOID12B/GEOID12B_CONUS.shtml\")\n",
    "        import wget\n",
    "        wget.download(\"https://www.ngs.noaa.gov/PC_PROD/GEOID12B/Format_unix/g2012bu0.bin\")\n",
    "    geoidHeight = instance.height(lats, lons)\n",
    "    return ellipsoids - geoidHeight\n",
    "\n",
    "def load_yellowfin_NMEA_files(fpath, saveFname, plotfname=False, verbose=False):\n",
    "    \"\"\"loads and possibly plots NMEA data from Emlid Reach M2 on yellowin\n",
    "    \n",
    "    :param fpath: location to search for NMEA data files\n",
    "    :param saveFname: where to save the Hdf5 file\n",
    "    :param plotfname: where to save plot showing path of yellowfin, if False, will not plot (default=False)\n",
    "    :param verbose: will print more output when processing if True (default=False)\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    dd = glob.glob(os.path.join(fpath, '*.dat'))\n",
    "    if verbose: print(f'processing {len(dd)} GPS data files')\n",
    "    \n",
    "    ji = 0\n",
    "    gps_time, lat, lon, altWGS84, altMSL, pc_time_gga = [], [], [], [], [], []\n",
    "    lat, latHemi, lon, lonHemi, fixQuality, satCount, HDOP = [], [], [], [], [], [], []\n",
    "    elevationMSL, eleUnits, geoSep, geoSepUnits, ageDiffGPS, diffRefStation = [], [], [], [], [], []\n",
    "    for fi in tqdm.tqdm(range(1, len(dd))):\n",
    "        fname = dd[fi]\n",
    "        \n",
    "        if verbose: print(fname)\n",
    "\n",
    "        with open(fname, 'r') as f:\n",
    "            lns = f.readlines()\n",
    "        \n",
    "        for ln in lns:\n",
    "            if ln.strip():\n",
    "                try:\n",
    "                    ss = ln.split('$')\n",
    "                    datestring = ss[0].strip('#')\n",
    "                    stringNMEA = ss[1].split(',')\n",
    "                except IndexError:\n",
    "                    continue\n",
    "                \n",
    "                dt = DT.datetime.strptime(datestring, '%Y-%m-%d %H:%M:%S.%f')\n",
    "                nmcode = stringNMEA[0]\n",
    "\n",
    "                if nmcode == 'GNGGA' and len(stringNMEA[2]) > 1:\n",
    "                    # Sentence Identifier: This field identifies the type of NMEA sentence and is represented by \"$GPGGA\" for the GGA sentence.\n",
    "                    # 1. UTC Time: This field provides the time in hours, minutes, and seconds in UTC.\n",
    "                    # 2. Latitude: This field represents the latitude of the GPS fix in degrees and minutes, in the format of\n",
    "                    #     ddmm.mmmm, where dd denotes degrees and mm.mmmm denotes minutes.\n",
    "                    # 3. Latitude Hemisphere: This field indicates the hemisphere of the latitude, either \"N\" for North or \"S\" for South.\n",
    "                    # 4. Longitude: This field represents the longitude of the GPS fix in degrees and minutes, in the format\n",
    "                    # of dddmm.mmmm, where ddd denotes degrees and mm.mmmm denotes minutes.\n",
    "                    # 5. Longitude Hemisphere: This field indicates the hemisphere of the longitude, either \"E\" for East or \"W\" for West.\n",
    "                    # 6. GPS Fix Quality: This field provides information about the quality of the GPS fix, represented by a\n",
    "                    # numeric value. Common values include 0 for no fix, 1 for GPS fix, and 2 for Differential GPS (DGPS) fix.\n",
    "                    # 7. Number of Satellites in Use: This field indicates the number of satellites used in the GPS fix represented by a numeric value.\n",
    "                    # 8. Horizontal Dilution of Precision (HDOP): This field represents the HDOP, which is a measure of the\n",
    "                    # horizontal accuracy of the GPS fix, represented by a numeric value.\n",
    "                    # 9 Altitude: This field provides the altitude above mean sea level (MSL) in meters, represented by a numeric value.\n",
    "                    # 10 Altitude Units: This field indicates the units used for altitude, typically \"M\" for meters.\n",
    "                    # 11 Geoidal Separation: This field represents the geoidal separation, which is the difference between\n",
    "                    # the WGS84 ellipsoid and mean sea level, in meters, represented by a numeric value.\n",
    "                    # 12Geoidal Separation Units: This field indicates the units used for geoidal separation, typically \"M\" for meters.\n",
    "                    # 13 Age of Differential GPS Data: This field provides the age of the DGPS data used in the GPS fix, represented by a numeric value.\n",
    "                    # 14 Differential Reference Station ID: This field indicates the identification number of the DGPS\n",
    "                    # reference station used in the GPS fix, represented by a numeric value.\n",
    "                    #\n",
    "                    # parse the individual string, add to list\n",
    "                    pc_time_gga.append(dt)\n",
    "                    gps_time.append(float(stringNMEA[1]))\n",
    "                    lat.append(float(stringNMEA[2][:2]) + float(stringNMEA[2][2:]) / 60)\n",
    "                    latHemi.append(stringNMEA[3])\n",
    "                    lona = float(stringNMEA[4][:3]) + float(stringNMEA[4][2:]) / 60\n",
    "                    lonHemi.append(stringNMEA[5])\n",
    "                    if lonHemi == 'W': lona = -lona\n",
    "                    lon.append(lona)\n",
    "                    fixQuality.append(\n",
    "                            int(stringNMEA[6]))  # GPS Fix Quality: represented by anumeric value. Common values\n",
    "                    # include 0 for no fix, 1 for GPS fix, and 2 for Differential GPS (DGPS) fix.\n",
    "                    satCount.append(int(stringNMEA[7]))\n",
    "                    HDOP.append(float(stringNMEA[8]))  # measure of the horizontal accuracy of the GPS fix, represented\n",
    "                    # by a numeric value.\n",
    "                    altMSL.append(float(stringNMEA[9]))\n",
    "                    eleUnits.append(stringNMEA[10])\n",
    "                    geoSep.append(float(stringNMEA[11]))\n",
    "                    geoSepUnits.append(stringNMEA[12])\n",
    "                    ageDiffGPS.append(float(stringNMEA[13]))\n",
    "                    diffRefStation.append(stringNMEA[14].strip())\n",
    "\n",
    "    lat = np.array(lat)\n",
    "    lon = np.array(lon)\n",
    "    lat[lat == 0] = np.nan\n",
    "    lon[lon == 0] = np.nan\n",
    "    # convert datetimes to epochs for file writing.\n",
    "    gpstimeobjs = [DT.time(int(str(ii)[:2]), int(str(ii)[2:4]), int(str(ii)[4:6]), int(str(ii)[7:] + '00000')) for ii in\n",
    "                   gps_time]\n",
    "    aa = [DT.datetime.combine(pc_time_gga[ii].date(), gpstimeobjs[ii]) for ii in range(len(gpstimeobjs))]\n",
    "    # now save output file\n",
    "    with h5py.File(saveFname, 'w') as hf:\n",
    "        hf.create_dataset('lat', data=lat)\n",
    "        # hf.create_dataset('latHemi', data=latHemi)\n",
    "        hf.create_dataset('lon', data=lon)\n",
    "        # hf.create_dataset('lonHemi',data=lonHemi)\n",
    "        hf.create_dataset('fixQuality', data=fixQuality)\n",
    "        hf.create_dataset('satCount', data=satCount)\n",
    "        hf.create_dataset('HDOP', data=HDOP)\n",
    "        hf.create_dataset('pc_time_gga', data=[mLabDatetime_to_epoch(pc_time_gga[ii]) for ii in range(len(pc_time_gga))])\n",
    "        hf.create_dataset('gps_time', data=[mLabDatetime_to_epoch(aa[i]) for i in range(len(aa))])\n",
    "        hf.create_dataset('altMSL', data=altMSL)\n",
    "        # hf.create_dataset('eleUnits', data=eleUnits) # putting time as first axis\n",
    "        # hf.create_dataset('geoSepUnits', data=geoSepUnits)\n",
    "        hf.create_dataset('ageDiffGPS', data=ageDiffGPS)\n",
    "    # now plot data\n",
    "    if plotfname is not False:\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(lon, lat, '-')\n",
    "        plt.subplot(122)\n",
    "        # plt.plot(pc_time_gga, altWGS84, '.-')\n",
    "        # plt.plot(pc_time_gga, geoSep, label='geoSep')\n",
    "        plt.plot(pc_time_gga, altMSL, '.-', label='altMSL')\n",
    "        plt.savefig(plotfname)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "def findTimeShiftCrossCorr(signal1, signal2, sampleFreq=1):\n",
    "    \"\"\"  Finds time shift between two signals.\n",
    "    \n",
    "    :param signal1: a signal of same length of signal 2\n",
    "    :param signal2: a signal of same length of signal 1\n",
    "    :param sampleFreq: sampling frequency, in HZ\n",
    "    :return: phase lag in samples, phase lag in time\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    from scipy.signal import correlate\n",
    "    assert len(signal1) == len(signal2), 'signals need to be the same lenth'\n",
    "    # load your time series data into two separate arrays, let's call them signal1 and signal2.\n",
    "    # compute the cross-correlation between the two signals using the correlate function:\n",
    "    cross_corr = correlate(signal1, signal2)\n",
    "    # Identify the index of the maximum value in the cross-correlation:\n",
    "    max_index = np.argmax(np.abs(cross_corr))\n",
    "    # Compute the phase lag in terms of the sample offset:\n",
    "    phase_lag_samples = max_index - (len(signal1) - 1)\n",
    "    # # If desired, convert the phase lag to time units (e.g., seconds):\n",
    "    phase_lag_seconds = phase_lag_samples * sampleFreq\n",
    "    return phase_lag_samples, phase_lag_seconds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b0bf16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
